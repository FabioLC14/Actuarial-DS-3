{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "module3-lab3-mlops.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "2809FQ0wLv2_",
        "LJAn8rGjoEv1",
        "1nocYJyfoEtO",
        "IOyuaFgeoElK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> <span style=\"color:black\"> IA|BE Data Science Certificate - Module 3 - Computer lab 3  </h1> </center>\n",
        "\n",
        "\n",
        "\n",
        "<center> <h2> <span style=\"color:red\"> MLOps workflow and popular tools</h1> </center>"
      ],
      "metadata": {
        "id": "iXXeOjgfn_zQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLOps (Machine Learning Operations) is loosely defined as a set of practices that aims to deploy and maintain ML models in production. MLOps lies at the intersection of Machine Learning, Data Engineering and DevOps. MLOps *provides an end-to-end machine learning development process to design, build and manage reproducible, testable, and evolvable ML-powered software* [(ml-ops.org)](https://ml-ops.org).\n",
        "\n",
        "Some key elements to introduce MLOps:\n",
        "* DevOps deals with code, MLOps deals with code, data, models and their interactions [(motivation)](https://ml-ops.org/content/motivation)\n",
        "* Stop thinking about models, start thinking about workflows [(phase 0)](https://ml-ops.org/content/phase-zero)\n",
        "* One step deeper even, start thinking about end-to-end workflows [(E2E workflow)](https://ml-ops.org/content/end-to-end-ml-workflow)\n",
        "* MLOps puts focus on elements such as automation, continuous X (CI/CD), versioning, experiment tracking, testing and monitoring [(principles)](https://ml-ops.org/content/mlops-principles)\n",
        "\n",
        "This workshop will cover the following elements from the MLOps realm:\n",
        "* **Version Control** \n",
        "  * goal: keep track of every modification to your code in a special kind of database\n",
        "  * tools: git and GitHub\n",
        "* **Containers**\n",
        "  * goal: run your software application reliably when moved from one computing environment to another\n",
        "  * tools: Docker and Docker Hub\n",
        "* **ML Workflows**\n",
        "  * goal: orchestration and scheduling of all the different steps in your ML process\n",
        "  * tool: Apache Airflow\n",
        "\n",
        "A lot of tools will be introduced and used in this workshop, so the first thing to do is install all of these such that you can follow along on your own machine. The following section walks you through the different steps, good luck!\n",
        "\n",
        "Most of this workshop will take place in your command line tool, for example the Terminal (macOS) or PowerShell (Windows). By default (if not specified differently), all commands should be run there. "
      ],
      "metadata": {
        "id": "sr51JidooEyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Technical prerequisites"
      ],
      "metadata": {
        "id": "2809FQ0wLv2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section lists the installation process for all the tools that we will be using in this workshop."
      ],
      "metadata": {
        "id": "pmhOoMspgnrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Anaconda (optional)"
      ],
      "metadata": {
        "id": "FJydy9ZYghGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section covers the installation of Anaconda Distribution:\n",
        "* **Installation**: select the appropriate [installer](https://www.anaconda.com/products/distribution#Downloads) based on your operating system\n",
        "* **Documentation**: the process is fairly straightforward, but additional [documentation](https://docs.anaconda.com/anaconda/install/) is available for each operating system\n",
        "\n",
        "Anaconda Distribution installs Python, package managers (pip & conda) and numerous Data Science Python packages like numpy, pandas, matplotlib, scikit-learn and more."
      ],
      "metadata": {
        "id": "LzqpF1NHgg9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test that both python and pip are installed by running the following commands in your Terminal (macOS) or PowerShell (Windows):\n",
        "* `pip --version`\n",
        "* `python --version` (you might need to try `py --version` in Windows)"
      ],
      "metadata": {
        "id": "AIJiJ72olLjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install git"
      ],
      "metadata": {
        "id": "Rt7i-zZXL42P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section covers the installation of git for different operating systems:\n",
        "\n",
        "* **Windows**:\n",
        "  * download the [.exe file](https://git-scm.com/download/win)\n",
        "  * run the installation wizard\n",
        "* **macOS**:\n",
        "  * open your terminal\n",
        "  * install [homebrew](https://brew.sh) by running the following command from the homebrew link in the terminal:\n",
        "    * `/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"` \n",
        "  * install git via homebrew by running `brew install git` in the terminal\n",
        "* **Linux**:\n",
        "  * choose the appropriate way of installation based on your distribution from [here](https://git-scm.com/download/linux)"
      ],
      "metadata": {
        "id": "hnWGp-iEMUsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test that git is installed by running the following command in your Terminal (macOS) or PowerShell (Windows):\n",
        "* `git --version`"
      ],
      "metadata": {
        "id": "x-C_BQReolSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create GitHub account and generate your PAT"
      ],
      "metadata": {
        "id": "RkkrxpVfL40S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section covers the setup of your GitHub account:\n",
        "\n",
        "* Create a GitHub account via the [sign-up](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F&source=header-home) page\n",
        "* Once signed in to your account, generate your personal access token (PAT) by following this [process](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\n",
        "  * Be sure to save your PAT somewhere, because you will only be able to see it this one time.\n",
        "  * You need this PAT when you are performing git actions via the command line that require authentication. So when GitHub ask for your password you simply provide your PAT. This is required since GitHub increased the security of such actions and they no longer support password for this, only PATs."
      ],
      "metadata": {
        "id": "dnyAvei4L4x0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Docker"
      ],
      "metadata": {
        "id": "3JkmFvRwL4vK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section covers the installation of Docker Desktop for different operating systems:\n",
        "\n",
        "* **Windows**:\n",
        "  * download the [.exe file](https://docs.docker.com/desktop/windows/install/) and run the installation wizard\n",
        "* **macOS**:\n",
        "  * download the [.dmg file](https://docs.docker.com/desktop/mac/install/) and run the installation wizard\n",
        "* **Linux**:\n",
        "  * choose the appropriate way of installation based on your distribution from [here](https://docs.docker.com/desktop/linux/install/)\n",
        "  * if your distribution does not support Docker Desktop yet, then you can install Docker Engine Server from [here](https://docs.docker.com/engine/install/)"
      ],
      "metadata": {
        "id": "MD94fWC4L4mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test that Docker is installed by running the following command in your Terminal (macOS) or PowerShell (Windows):\n",
        "* `docker --version`"
      ],
      "metadata": {
        "id": "BKyzxSDFp1N1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Docker Desktop normally includes Docker Compose, which we will be using during this workshop to run Airflow. Make sure that Docker Compose is installed by running the command below in your Terminal (macOS) or PowerShell (Windows). If Docker compose is not installed, you can follow these [instructions](https://docs.docker.com/compose/install/) based on your operating system.\n",
        "* `docker-compose --version`"
      ],
      "metadata": {
        "id": "J5LUeZPzuq-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Docker Hub account"
      ],
      "metadata": {
        "id": "qjHCSWpPYbsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section covers the setup of your Docker Hub account:\n",
        "\n",
        "* Create a Docker Hub account via the [sign-up](https://hub.docker.com/signup) page"
      ],
      "metadata": {
        "id": "iVT4ud2TYbjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Airflow"
      ],
      "metadata": {
        "id": "ZNzBrFukLvii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could try to install Airflow locally by following the following [instructions](https://airflow.apache.org/docs/apache-airflow/stable/start/local.html). However, a local installation of Airflow runs into errors many times and takes a lot of effort to fix and maintain over time due to frequently updated dependencies. For that reason we will leverage Docker (the tool you just installed and will learn more about during the workshop) to demonstrate the use of Airflow. So no installation needed so far!\n",
        "\n",
        "If you tried the local install, then run the following commands in your Terminal (macOS) or PowerShell (Windows) to verify your success:\n",
        "* `airflow version` (without the double dash this time)\n",
        "* `airflow standalone` to launch the Airflow webserver\n",
        "\n",
        "Do note that this workshop will not use the local installation of Airflow, so no worries if the above airflow commands don't work. \n"
      ],
      "metadata": {
        "id": "TraFsp3SgcOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You are all set and good to go for the workshop, have fun! 🚀"
      ],
      "metadata": {
        "id": "9utdDkb6x8HH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version Control with Git and GitHub"
      ],
      "metadata": {
        "id": "LJAn8rGjoEv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing a large project in your company with a lot of contributors comes with a lot of challenges:\n",
        "* efficient collaboration among people\n",
        "* storing different versions of the project\n",
        "* being able to rollback to previous versions and backup in case of system failure\n",
        "* figuring out why v1 worked well, but v2 results in errors all over the place\n",
        "\n",
        "Version Control to the rescue! The basic idea of version control is to track changes in your (and your collaborators) files. VC systems set up all the technical details needed for doing exactly that. This comes with a lot of benefits:\n",
        "* managing and protecting the developers' source code by keeping track of all source code changes\n",
        "* allowing collaboration among developers such that they are always working on the latest version of the code\n",
        "* go back in time and figure out what changed in the source code to troubleshoot issues\n",
        "* always have a backup available with the total history of the project\n",
        "\n",
        "This sections provides an introduction to version control with git and GitHub, but what are these tools used for? **Git** is a local open-source tool that helps developers to manage source code. **GitHub** an online service where developers can connect to resources using git."
      ],
      "metadata": {
        "id": "vxj-nYNBoRK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure git"
      ],
      "metadata": {
        "id": "CY_A7dEAjn-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can configure git such that it (and other developers) know who you are and how to reach you by running the following terminal commands:\n",
        "\n",
        "* set your name: `git config --global user.name \"Your Name\"`\n",
        "* set your email: `git config --global user.email your_name@mail.com`\n",
        "* check the configuration: `git config -l`\n",
        "* another way to check the configuration via the .gitconfig file: `cat ~/.gitconfig`\n",
        "\n",
        "We used the `--global` tag to set these configurations globally over all your repositories. You can set the configuration for each repo seperately by navigating into this repo and running the command without this tag."
      ],
      "metadata": {
        "id": "gGZOzBS1jsoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic git commands"
      ],
      "metadata": {
        "id": "moOHozhtjsT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will cover the following git commands:\n",
        "* `git init`: initialize a git repo\n",
        "* `git status`: check the status of your repo\n",
        "* `git add`: stage files for git to start tracking changes\n",
        "* `git commit`: commit changes to your repo\n",
        "* `git log`: get a log overview of all your commits"
      ],
      "metadata": {
        "id": "4zkTOxnp6Rrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script can be followed to play along with some basic git operations:\n",
        "\n",
        "* `cd Desktop`: go to your desktop directory (might be different depending on where your command line tool situates you)\n",
        "* `mkdir demo-git`: create an empty directory \"demo-git\"\n",
        "* `cd demo-git`: got into that empty directory \"demo-git\"\n",
        "* `ls`: list all the files to verify that the directory is empty\n",
        "* `git init`: **initialize a new repository inside \"demo-git\" (this creates a .git directory in the repo used for git metadata)**\n",
        "* `ls -a`: list all files (also hidden ones)\n",
        "* `git status`: **so far nothing to commit or track**\n",
        "* `echo \"hi there\" > hello.txt`: create a simple hello.txt file with the text \"hello there\"\n",
        "* `git status`: **git tells us that we have one untracked file (hello.txt)**\n",
        "* `git add hello.txt`: **tell git to track changes in this file**\n",
        "* `git status`: **git tells us that we have one new file being tracked (hello.txt)**\n",
        "* `git commit -m \"our first commit\"`: **this commits our staged changes to the repo**\n",
        "* `git log`: **shows an overview of all your commits**\n"
      ],
      "metadata": {
        "id": "SeSVYofFjtPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Branching and merging"
      ],
      "metadata": {
        "id": "UsNX_bqpMLWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will cover the following git commands:\n",
        "* `git branch`: create a new branch in your repo\n",
        "* `git checkout`: switch to another branch in your repo\n",
        "* `git merge`: merge another branch into your current branch"
      ],
      "metadata": {
        "id": "64OCfwF-6xnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script can be followed to see the effect of branches and merges:\n",
        "\n",
        "* `git branch dev-test`: **create a new branch with the name \"dev-test\"**\n",
        "* `git checkout dev-test`: **switch the git HEAD to our new branch**\n",
        "* `git log`: **notice that our original commit to master is also part of this new branch**\n",
        "* `echo \"general kenobi\" > hello2.txt`: create a new hello2.txt file\n",
        "* `git add .`: **stage these changes for git to track**\n",
        "* `git commit -m \"second commit\"`: **commit these changes on our dev-test branch**\n",
        "* `git log`: **we now see 2 commits in our logs**\n",
        "* `git checkout master`: **switch back to the master branch**\n",
        "* `git log`: **we only see our first commit on the master branch**\n",
        "* `git merge dev-test`: **merge the dev-test branch into the master branch**\n",
        "* `git log`: **now we see both commits in the master branch, great!**"
      ],
      "metadata": {
        "id": "ttWibLoNMLOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remote repositories on GitHub"
      ],
      "metadata": {
        "id": "bCaj2jEN3Xvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will cover the following git commands:\n",
        "* `git clone`: clone a remote repo to your local filesystem\n",
        "* `git push`: push changes from your local repo to the remote repo\n",
        "* `git pull`: pull changes from a remote repo to your local repo"
      ],
      "metadata": {
        "id": "79O4CPTr7LBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create our very first GitHub repo:\n",
        "\n",
        "* On your GitHub accout homepage, go to the tab \"Repositories\" and click the green \"New\" button\n",
        "* Give your repo a name \"demo-test\" and (optional) description\n",
        "* Optionally you can add a README file, .gitignore template and a license\n",
        "* Click \"Create repository\"\n",
        "\n",
        "Cool! We now have our repo publicly available on GitHub."
      ],
      "metadata": {
        "id": "9YHBL3FB3Xks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now \"clone\" this repo onto our local machine to start working with it following these steps:\n",
        "\n",
        "* `cd ~/Desktop`: we go to our desktop folder and will clone our git repo here\n",
        "* `git clone https://github.com/rhen-pl/demo-test.git`: **this command clones the remote repo to our desktop**\n",
        "  * replace my username \"rhen-pl\" with your username if you created a repo on GitHub\n",
        "  * you can optionally specify another path at the end of this command for clone destination (default to your work dir)\n",
        "  * the link we used can be found via \"code > HTTPS\" on your remote repo page\n",
        "* `cd demo-test`: let us move into our local repo\n",
        "* `git status`: **our branch is up to date with 'origin/main'**\n",
        "* `echo \"test\" > test.txt`: create a simple text file in your local repo\n",
        "* `git status`: **we now have untracked changes like we have seen before**\n",
        "* `git add .`: **stage your changes**\n",
        "* `git commit -m \"my first commit\"`: **commit your changes**\n",
        "* `git status`: **our branch is ahead of 'origin/main' by 1 commit**\n",
        "* `git push origin main`: **push your local changes into the remote repo**\n",
        "  * GitHub will ask for your username and password\n",
        "  * Be sure to put your PAT as password\n",
        "  * If you now go to your remote GitHub repo, you will see the \"test.txt\" file. Cool, right?!\n",
        "* `git pull`: **pull changes from the remote repo to your local one**\n",
        "  * git will tel you *Already up to date.*\n",
        "  * This makes sense since we just pushed our local changes into the remote repo\n",
        "  * Let's add a file by clicking *Add file* in the remote repo on GitHub\n",
        "  * Specify the name (something.txt) with some random content and click *Commit new file*\n",
        "* `git pull`: **now we pulled that new file from the remote repo to the local one**\n",
        "  * You should see this file in your local directory now\n",
        "  * `ls` to list the files, is it there?\n",
        "  * `cat something.txt` to see the content of the file"
      ],
      "metadata": {
        "id": "xhgpEH6n3WeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if you want to start experimenting with some publicly available repo on GitHub? You won't be able to clone from and push to someone's work without their permission. To solve this issue, you can \"Fork\" any GitHub repo:\n",
        "\n",
        "* Go to the page of the repo with which you want to experiment\n",
        "* Click the \"Fork\" button on the top right of the page\n",
        "* This create a copy of this repo into your own repository list\n",
        "* Now you can experiment freely without affecting the original owner's work"
      ],
      "metadata": {
        "id": "adDKsB9S8X7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Containers with Docker and Docker Hub"
      ],
      "metadata": {
        "id": "1nocYJyfoEtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At some point in time, the code you have written in **development** will move to **production**. Now let's imagine the following situation:\n",
        "\n",
        "> You're a programmer and you've just written some code. It seems to work fine locally, all your unit tests pass and everything. Time to put stuff to production! So you contact your infrastructure team to set up a production server for you, install an OS there, configure the environment, install dependencies, and so on. After that you finally deploy your code to prod. Hooray, right? Wrong! You see you developed in a Windows environment, and the server machine is running on Linux. And because of environment differences, your unit tests now fail...\n",
        "\n",
        "This unfortunately happens from time to time (especially in the past) until **virtual machines** came into existence. A virtual machine (VM) is a computer inside your computer. A VM emulates a whole system from hardware up, so you get your own CPU power, hard drive space and everything on top - an operating system with all its processes, anything you wish to install on it, your programming environment etc. Having one virtual machine per process/environment is the perfect way to isolate that process/environment. However, VMs are expensive to store, to run, and to modify. Enter containers!\n",
        "\n",
        "For all intents and purposes, you can think of a **container** as a lighter, faster, much easier (to spin up, operate and change) version of VMs. Containers will share the underlying operating system resources, but above that the environment will be completely isolated. Because containers are lightweight and easy to build, they're also much easier to share than VMs, so you can ensure everyone in your team works in exactly the same environment. Or that the environment on your machine is identical to the one on the production server so you can be sure that test run locally during development will give an accurate execution result in production."
      ],
      "metadata": {
        "id": "OCm1ilYkgJf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3 basic building blocks to container based development: an **image**, a **container**, and a container **engine**. \n",
        "* an ***image*** is a snapshot of your runtime environment. Think a powered down virtual machine. It has all the prerequisites that you need to run your code: the environment, dependencies, the code itself and instructions on how to run the code when it's booted.\n",
        "* A ***container*** is a running instance of an image, so basically an up and running VM.\n",
        "* A container ***engine*** is software that runs and manages images in an environment (e.g., Docker Desktop or Kubernetes Container Engine).\n",
        "\n",
        "This section provides an introduction to containers with Docker. Be sure to start Docker Desktop before continuing."
      ],
      "metadata": {
        "id": "sR6RzCvjoQqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hello Docker"
      ],
      "metadata": {
        "id": "1qGkhLPdo41s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following command in your terminal and let's observe what happens: `docker run hello-world`\n",
        "\n",
        "Wow, look at that! You just interacted with your first Docker image and spun up a container from that image. But how could we access this image without having it on our local machine? Docker actually tells us this in the output:\n",
        "\n",
        "> Unable to find image 'hello-world:latest' locally\n",
        "\n",
        "> latest: Pulling from library/hello-world\n",
        "\n",
        "> 7050e35b49f5: Pull complete \n",
        "\n",
        "Docker pulled the official `hello-world` [image](https://hub.docker.com/_/hello-world) from Docker Hub. The output list the steps which are taken by Docker under the hood:\n",
        "1. The Docker client contacted the Docker daemon.\n",
        "2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n",
        "3. The Docker daemon created a new container from that image which runs the\n",
        "    executable that produces the output you are currently reading.\n",
        "4. The Docker daemon streamed that output to the Docker client, which sent it\n",
        "    to your terminal.\n"
      ],
      "metadata": {
        "id": "1BNOyFFNRVOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing images and containers"
      ],
      "metadata": {
        "id": "yFExrFd3RXMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try the following commands in the terminal to list images and containers:\n",
        "* `docker image ls`: list all your Docker **images**\n",
        "  * do you spot the hello-world image?\n",
        "* `docker container ls`: list all your **running containers**\n",
        "  * can you see the hello-world container? Why (not)?\n",
        "* `docker container ls -a`: list **all containers**, also the stopped ones\n",
        "  * now we can see the hello-world container listed"
      ],
      "metadata": {
        "id": "OybNijyLRXF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hello Python"
      ],
      "metadata": {
        "id": "1J-jKDIkady7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following command in your terminal and let's observe what happens: `docker run python`\n",
        "\n",
        "This will pull the offical Python [image](https://hub.docker.com/_/python) from Docker Hub (note that this download might take a minute). Once this pull is completed, nothing seems to have happened? Let's list our images and containers:\n",
        "* `docker images ls`: we can see both the hello-world and python images\n",
        "* `docker container ls`: there seem to be 0 containers running\n",
        "* `docker container ls -a`: the python container is indeed in a stopped state\n",
        "\n",
        "The container executes the python3 command and then exits. If you want to run a container in interactive mode, run the following command: `docker run -it python` (with the interactive and tty flag).\n",
        "\n",
        "Now you arrive in an interactive Python prompt where you can execute Python commands:\n",
        "* `print('Hello there\")`\n",
        "* `1+2`\n",
        "* `[1,2,3]`\n",
        "* `exit()` to exit\n",
        "\n",
        "When you now list your containers, you should see two stopped python containers. Notice the weird random names that are assigned? This might get confusing fast once you have more containers from the same image to track. Run a container with a specific name as follows: `docker run --name my-python-container python`. Can you spot this one in your container listing?\n"
      ],
      "metadata": {
        "id": "HHn-FOhLadu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Container management"
      ],
      "metadata": {
        "id": "NkbbDLnpadpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have multiple containers from the python image. Run the `docker image ls` command to verify that there is still only one python image in our listing. All our python containers are seperate running instances from the same image.\n",
        "\n",
        "Let's clean up! Run `docker container prune` and confirm with `y`. This removes all stopped containers, but not the images. Confirm this yourself with the listing commands. Notes: \n",
        "* run `docker container prune --force` without the need for confirmation\n",
        "* run `docker run --rm python` if you want to remove the container automatically once it's stopped\n",
        "\n",
        "You can run a container in the background in detached mode as follows: `docker run -dt --name my-python python`.\n",
        "  * list the *running* containers with `docker container ls`, what do you see?\n",
        "  * stop the container via `docker stop my-python`\n",
        "  * restart the container via `docker restart my-python`\n"
      ],
      "metadata": {
        "id": "LpEYx3Q_RXDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images and the Dockerfile\n",
        "\n",
        "A Docker image is the source of a container, sort of a blueprint of what needs to happen at runtime. The Dockerfile can be seen as the step-by-step plan of instructions which specifies all the layers like code, runtime, libraries, variables and configurations in the image.\n"
      ],
      "metadata": {
        "id": "KOckoWssmB2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base images"
      ],
      "metadata": {
        "id": "zQjnfwCdmBzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [Dockerfile](https://github.com/docker-library/hello-world/blob/3332fbee4210b41738d83f6cfdc301a42b96e30d/amd64/hello-world/Dockerfile) from the hello-word image has the following three lines of code:\n",
        "```\n",
        "FROM scratch\n",
        "COPY hello /\n",
        "CMD [\"/hello\"]\n",
        "```\n",
        "* The first line `FROM scratch` indicates that this is a **base image** which literally starts from *scratch*. \n",
        "* Next, a hello binary that exists in the git [directory](https://github.com/docker-library/hello-world/tree/3332fbee4210b41738d83f6cfdc301a42b96e30d/amd64/hello-world) is copied in the root file system\n",
        "* Finally, the CMD is run when the container is spun up (and the container exits after executing this)"
      ],
      "metadata": {
        "id": "AAlUaJzdmBsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom image"
      ],
      "metadata": {
        "id": "VgScL-vzmBjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is of course no need to reinvent the wheel every time. You can easily base your image on another **parent image** and start building from there. An example Dockerfile is as follows:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# syntax=docker/dockerfile:1                            \n",
        "FROM python\n",
        "WORKDIR /my_awesome_app\n",
        "COPY requirements.txt requirements.txt\t\t\t\t\t\t\n",
        "RUN pip3 install -r requirements.txt.\n",
        "COPY . .\n",
        "CMD [\"python3\", \"hello_world.py\"]\n",
        "```\n",
        "* Let Docker know which version of syntax you're using\n",
        "* Start from the **python** image as parent image\n",
        "* Create folder **my_awesome_app** at the root of the image & make it a working directory\n",
        "* Copy dependency **requirements** from your local machine to the image\n",
        "* Install dependencies by running the **pip3 install** command\n",
        "* **Copy** your entire project into the image\n",
        "* Execute the command **hello_world.py** on container startup"
      ],
      "metadata": {
        "id": "_0yDa1_LqtGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building your image"
      ],
      "metadata": {
        "id": "ZEmeBm5fqtCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a directory \"demo-docker\" with the following three files and run `cd demo-docker`:\n",
        "* **Dockerfile** with the above commands\n",
        "* **requirements.txt** with *numpy* as package\n",
        "* **hello_world.py** with *print(\"Hello there\")*\n"
      ],
      "metadata": {
        "id": "hyzVyOa1L-Ik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following commands to build the image and run a container:\n",
        "* `docker build . -t my-awesome-image`: build the image in the directory where your Dockerfile is located\n",
        "  * if your path is not in this directory, the `.` needs to contain the path to the Dockerfile\n",
        "  * the `-t` flag is used to tag your image with a name\n",
        "* `docker image ls`: verify your image is built\n",
        "* `docker run --name my-awesome-container my-awesome-image`: run a container from your custom image"
      ],
      "metadata": {
        "id": "LItcMSTeqs-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Publishing your image"
      ],
      "metadata": {
        "id": "cL8pGaKVqs2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the follwoing commands:\n",
        "* `docker login --username=<username>`: login to Docker Hub by specifying your own username\n",
        "* `docker tag my-awesome-image <username>/my-awesome-repo:1.0`: tag your image for a repository\n",
        "* `docker push <username>/my-awesome-repo:1.0`: push your image to the repository\n",
        "  * now check your Docker Hub account, is the repo there?\n",
        "* `docker logout`: log out from Docker Hub"
      ],
      "metadata": {
        "id": "3V_iBSSIRW7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scheduling workflows with Apache Airflow"
      ],
      "metadata": {
        "id": "IOyuaFgeoElK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Airflow is an orchestration tool that lets you schedule, author and monitor **workflows** programmatically, aka via code. But what is a workflow? A workflow is a **Directed Acyclic Graph** (or **DAG**) with multiple **tasks** that can be executed independently. Directed means flow in a certain direction, while acyclic means that loops are not allowed in the flow. So, a DAG connects tasks together, specifies their relationship (what runs before what) and dependencies (task B is ran if task A is completed successfully)."
      ],
      "metadata": {
        "id": "st-gbiUboQMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Programmatic workflow"
      ],
      "metadata": {
        "id": "5xrU5tvbfNAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Programmatically, DAGs are python files (`.py`) stored in a `/dags` folder of your project. They are defined (and scheduled) as follows:\n",
        "```\n",
        "from airflow.models import DAG\n",
        "\n",
        "with DAG(\"ml_pipeline\", description='End-to-end ML pipeline example', schedule_interval='@daily') as dag:\n",
        "\n",
        "\texecute_your_pipeline_steps\n",
        "```\n",
        "\n",
        "Every step of a pipeline is either a **task** (a single operation) or a **task group** (a bunch of tasks executed in parallel). \n",
        "\n",
        "To create a task in AirFlow, you use an operator. There are multiple operator classes, with the default ones being PythonOperator (for executing Python code), EmailOperator (sending emails) and BashOperator (executing bash scripts). In code this looks as follows:\n",
        "\n",
        "```\n",
        "from airflow.models import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "\n",
        "def hello():\n",
        "\tprint('hello')\n",
        "\n",
        "with DAG(\"ml_pipeline\", description='End-to-end ML pipeline example', schedule_interval='@daily') as dag:\n",
        "\n",
        "\thello_task = PythonOperator(task_id = 'hello_task', python_callable = hello)\n",
        "\temail_task = EmailOperator(to=\"admin@example.com\",subject=\"hello ran\")\n",
        "\n",
        "\thello_task >> email_task \n",
        "```\n",
        "The last line specifies the order of execution of tasks, and that's how you chain tasks in general, i.e. first_task >> second_task >> … >> last_task. These will be executed sequentially: task2 with start running only after task1 is executed successfully. To run tasks in parallel, you can combine them into task groups as follows:\n",
        "\n",
        "```\n",
        "from airflow.models import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.utils.task_group import TaskGroup\n",
        "\n",
        "def hello():\n",
        "\tprint('hello')\n",
        "\n",
        "def bye():\n",
        "\tprint('bye')\n",
        "\n",
        "with DAG(\"ml_pipeline\", description='End-to-end ML pipeline example', schedule_interval='@daily') as dag:\n",
        "\n",
        "\t########task group########\n",
        "\twith TaskGroup('hello_bye') as hello_bye:\n",
        "\t\t\n",
        "\t\thello_task = PythonOperator(task_id = 'hello_task',python_callable = hello)\n",
        "\t\tbye_task = PythonOperator(task_id = 'bye_task', python_callable = bye)\n",
        "\t\n",
        "\temail_task = EmailOperator(to=\"admin@example.com\", subject=\"hello_bye ran\")\n",
        "\n",
        "\thello_bye >> email_task \n",
        "```"
      ],
      "metadata": {
        "id": "zLFxWZW_fM4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting Airflow"
      ],
      "metadata": {
        "id": "V-XUgfMFDP4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the following steps to launch Airflow via Docker Compose:\n",
        "* start Docker Desktop\n",
        "* `cd Desktop`: go to your desktop directory (not the Docker one)\n",
        "* `git clone https://github.com/ProphecyLabs/demo_airflow.git`: clone the Prophecy Labs demo_airflow git [repo](https://github.com/ProphecyLabs/demo_airflow)\n",
        "* `cd demo_airflow`: change into the airflow demo directory\n",
        "* `docker-compose -f docker-compose.yaml up -d`: launch the airflow container via Docker Compose\n",
        "* After docker-compose finishes, go to [localhost:8080](http://localhost:8080) in your browser to see the Airflow UI. Enter login and password (airflow/airflow).\n",
        "\n",
        "On Linux you might need to perform the following steps before running the `docker-compose` command:\n",
        "* `mkdir ./dags ./data ./models ./logs ./plugins`: create directories necessary for demo\n",
        "* `echo -e \"AIRFLOW_UID=$(id -u)\" > .env`: match Airflow UID to Docker UID"
      ],
      "metadata": {
        "id": "lCKejAmODPvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What just happened?"
      ],
      "metadata": {
        "id": "GzREBkMlJJO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're running an Airflow application, meaning you spun up:\n",
        "\n",
        "* Airflow webserver, the very tool at [localhost:8080](http://localhost:8080), giving you a nice UI to see job executions, logs, graphs of your pipelines etc.\n",
        "* Airflow triggerer, a tool to trigger the execution of your pipelines\n",
        "* Airflow scheduler, a tool to schedule pipeline execution\n",
        "* Airflow init, a platform initalization service\n",
        "* Airflow database, a DB that stores configurations such as variables, policies, links between tasks, permissions etc.\n",
        "\n",
        "The components above are essential to any Airflow run, and they all need to be present for Airflow to work. This is why you used docker-compose (runs multi-container apps) and not docker run (runs a single container).\n",
        "\n",
        "Other than spinning up Airflow, there're a few things the [docker-compose.yaml](https://github.com/ProphecyLabs/demo_airflow/blob/main/docker-compose.yaml) file did, namely:\n",
        "* created three folders in the root folder of your code repo, and mounted them to volumes inside your Airflow container:\n",
        "```\n",
        "volumes: \n",
        "\t- ./dags:/opt/airflow/dags\n",
        "    - ./data:/opt/airflow/data\n",
        "    - ./models:/opt/airflow/models\n",
        "```\n",
        "* Added a pip dependency for sklearn. Since we're building a ML pipeline it is reasonable to assume that we'll need to use some sort of ML library:\n",
        "```\n",
        "_PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-scikit-learn}\n",
        "```\n"
      ],
      "metadata": {
        "id": "Jz51sI59JJMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo time"
      ],
      "metadata": {
        "id": "gWriNlXJJJKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visit [localhost:8080](http://localhost:8080) to get access to the Airflow UI for the demo. We will cover the three DAGs in the [dags](https://github.com/ProphecyLabs/demo_airflow/tree/main/dags) repo folder.\n"
      ],
      "metadata": {
        "id": "Sj2F2j0dJJHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ml_pipeline"
      ],
      "metadata": {
        "id": "NtGPrZ6MNu63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will run this [DAG](https://github.com/ProphecyLabs/demo_airflow/blob/main/dags/ml_pipeline.py).\n",
        "\n",
        "* Click on the ml-pipeline DAG to see the different tasks in this pipeline (either in Tree or Graph view)\n",
        "* Trigger the pipeline by clicking on the play-button in the top-right and then \"trigger DAG\"\n",
        "* Now you can follow the execution of the pipeline in real time\n",
        "* After completion, you can visualise the time evolution of the different tasks in the Gantt tab\n",
        "* After completion, you can access the logs of each task by clicking on the task and then \"Log\"\n"
      ],
      "metadata": {
        "id": "bfmvPtTaNvr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### broken_code"
      ],
      "metadata": {
        "id": "wHVqBlReNvjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will (try to) run this [DAG](https://github.com/ProphecyLabs/demo_airflow/blob/main/dags/broken_code.py) but fail due to code error.\n",
        "\n",
        "* Click on the broken_code DAG to see the different tasks in this pipeline (either in Tree or Graph view)\n",
        "* Trigger the pipeline by clicking on the play-button in the top-right and then \"trigger DAG\"\n",
        "* Now you can follow the execution of the pipeline in real time\n",
        "* Autch, we seem to have a failed run\n",
        "* In the logs we can find the root cause: *NameError: name 'np' is not defined*"
      ],
      "metadata": {
        "id": "SDWvzcH2NvX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### broken DAG"
      ],
      "metadata": {
        "id": "4I5wAbG_RHLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can't run (or even show) this [DAG](https://github.com/ProphecyLabs/demo_airflow/blob/main/dags/broken_dag.py) because there is a DAG error. Airflow tells us this in the top of the UI. See the full stack trace as follows:\n",
        "* Open Docker Desktop\n",
        "* Navigate to Containers > demo_airflow > any container > CLI\n",
        "* `cd dags`: go into dags folder\n",
        "* `python broken_dag.py`: run the broken DAG\n",
        "* See the error *NameError: name 'DAG' is not defined*\n",
        "\n"
      ],
      "metadata": {
        "id": "_8X2UHA7RHB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Terminate the Airflow demo\n",
        "\n",
        "Run the command `docker-compose down` to temrinate the Airflow server in Docker Compose."
      ],
      "metadata": {
        "id": "4LhO5ZHSTCMk"
      }
    }
  ]
}